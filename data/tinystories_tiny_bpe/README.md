# TinyStories Dataset with Tiny BPE Tokenizer

## Why TinyStories? ⭐

**Perfect for small embedded language models!**

- ✅ **Clean English**: Simple vocabulary, no Unicode issues
- ✅ **Small Model Friendly**: Designed for models with <10M parameters
- ✅ **High Quality**: Generated by GPT-3.5/4, human-validated
- ✅ **Large Dataset**: ~2.1M stories, plenty of training data
- ✅ **Simple Grammar**: Uses basic sentence structures

## Dataset Information

- **Source**: [HuggingFace - roneneldan/TinyStories](https://huggingface.co/datasets/roneneldan/TinyStories)
- **Version**: default (main branch)
- **License**: Apache 2.0
- **Size**: ~2.1M stories (~1B tokens)
- **Language**: Simple English (vocabulary suitable for 3-4 year olds)

## Dataset Details

TinyStories contains short stories (average 200-300 words) using simple vocabulary and grammar. Each story is coherent and follows basic narrative structure.

**Example Story:**
```
Once upon a time, there was a little girl named Lily.
She loved to play outside in the sunshine. One day,
she saw a big, red ball in the park. "Can I play with
your ball?" she asked a boy. "Yes, you can!" he said.
They played together all day and became good friends.
```

**Vocabulary Characteristics:**
- Common words: ~1,500 unique words
- Simple grammar structures
- No complex punctuation
- Minimal abbreviations
- ASCII-friendly (no Unicode issues!)

## Comparison with Other Datasets

| Dataset | Vocab Complexity | Unicode Issues | Size | Best For |
|---------|-----------------|----------------|------|----------|
| **TinyStories** | ⭐ Simple | ✅ None | ~1B tokens | Small LMs (recommended!) |
| Shakespeare | Medium | ⚠️ Some | ~1M chars | Poetic text |
| WikiText-2 | Complex | ❌ Many | ~2M tokens | General LM |
| WikiText-103 | Complex | ❌ Many | ~103M tokens | Large LM |

## Usage

### 1. Install Dependencies

```bash
pip install datasets numpy
# or
uv pip install datasets numpy
```

### 2. Prepare Dataset

```bash
cd PicoGPT-SDK/data/tinystories_tiny_bpe
python prepare.py
```

**Configuration** (edit `prepare.py`):
```python
TARGET_VOCAB_SIZE = 256      # Vocabulary size
MAX_TRAIN_SAMPLES = 100000   # Limit samples (None = all 2.1M)
MAX_VAL_SAMPLES = 10000      # Limit validation samples
```

**Output:**
```
✓ train.bin  - Training tokens
✓ val.bin    - Validation tokens
✓ meta.pkl   - Vocabulary and tokenizer info
```

### 3. Train PicoGPT

```bash
cd ../../
python train_tf.py data/tinystories_tiny_bpe --out_dir=out-tf-tinystories
```

## Memory Usage

**Sample limits** (to avoid memory issues):

| Samples | Memory | Dataset Size | Training Time |
|---------|--------|--------------|---------------|
| 10,000 | ~500MB | ~5M chars | Fast (1-2h) |
| 100,000 | ~2GB | ~50M chars | Medium (5-10h) |
| 2,100,000 | ~20GB | ~1B chars | Slow (2-3 days) |

**Recommended for embedded:**
- Start with 10,000-100,000 samples
- Verify model quality
- Scale up if needed

## Expected Results

With TinyStories, your PicoGPT model should learn to:
- Generate simple, coherent sentences
- Use basic grammar correctly
- Create short story narratives
- Handle common vocabulary

**Example generation:**
```
Prompt: "Once upon a time"
Output: "Once upon a time there was a little boy.
         He liked to play with his dog. They were
         very happy together."
```

## Dataset Citation

```bibtex
@article{eldan2023tinystories,
  title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author={Eldan, Ronen and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023}
}
```

## Troubleshooting

**Out of Memory Error:**
```python
# Edit prepare.py:
MAX_TRAIN_SAMPLES = 10000  # Reduce this
```

**Download Timeout:**
The first run downloads ~5GB. Be patient or use a local copy.

**Alternative: Use pre-tokenized version:**
```bash
# Download from HuggingFace Hub (if available)
huggingface-cli download roneneldan/TinyStories-Instruct
```

## Why Not WikiText?

❌ **WikiText Issues:**
- Complex vocabulary (encyclopedic)
- Unicode characters: ", —, ×, café
- Technical terminology
- Requires larger vocab (512-1024)

✅ **TinyStories Advantages:**
- Simple vocabulary (256 vocab works!)
- Pure ASCII
- Better for small models
- Designed for this use case
